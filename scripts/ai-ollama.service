[Unit]
Description=Ollama LLM Server (Mistral models) for AI Agent
After=network-online.target
Wants=network-online.target

[Service]
Type=simple
WorkingDirectory=/var/www/clients/client1/web64/web
ExecStart=/bin/bash /var/www/clients/client1/web64/web/scripts/start_ollama.sh
User=web64
Group=client1
Restart=on-failure
RestartSec=5
Environment=OLLAMA_HOST=127.0.0.1
Environment=OLLAMA_PORT=11434
Environment=OLLAMA_HOME=/var/www/clients/client1/web64/web/ollama_home
LimitNOFILE=65536
# Logging handled inside start_ollama.sh (stdout/stderr redirected there)
NoNewPrivileges=true
PrivateTmp=true
ProtectSystem=full
ProtectHome=true

[Install]
WantedBy=multi-user.target
